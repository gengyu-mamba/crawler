# 导读：爬虫介绍

在正式学习 Python爬虫 之前，我们应该对以下内容有所了解。

一、爬虫是什么

网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。

二、爬虫的基本流程

1、获取数据
通过HTTP库向目标站点发起请求，即发送一个Request，等待服务器响应。如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容。

2、解析数据
将获取的页面内容，可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。

3、提取数据
爬虫的关键。从解析后的数据中筛选出我们需要的数据。

4、存储数据
将筛选的数据存储起来，便于后续的数据分析。可存储到特定格式的文件或数据库中。

三、爬虫的道德规范 - robots协议

编写爬虫程序爬取数据之前，为了避免某些有版权的数据后期带来的诸多法律问题，可以通过查看网站的robots.txt文件来避免爬取某些网页。robots协议，告知爬虫等搜索引擎哪些页面可以抓取，哪些不能。它只是一个通行的道德规范，没有强制性规定，完全由个人意愿遵守。作为一名有道德的技术人员，遵守robots协议，有助于建立更好的互联网环境。

网站的robots文件地址通常为网页主页后加robots.txt，如 [https://github.com/robots.txt](https://github.com/robots.txt)，我们截取了部分协议，内容如下：

```python
# If you would like to crawl GitHub contact us at support@github.com.
# We also provide an extensive API: https://developer.github.com/

User-agent: Googlebot # 谷歌搜索引擎
Allow: /*/*/tree/master
Allow: /*/*/blob/master
Disallow: /ekansa/Open-Context-Data
Disallow: /ekansa/opencontext-*

User-agent: Bingbot # Bing搜索引擎
Allow: /*/*/tree/master
Allow: /*/*/blob/master
Disallow: /ekansa/Open-Context-Data
Disallow: /ekansa/opencontext-*

User-agent: * # 其他所有的搜索引擎种类
Allow: /humans.txt
Disallow: /
```

协议里的Allow和Disallow，Allow代表可以被访问，Disallow代表禁止被访问。












